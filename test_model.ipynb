{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Embedding,Bidirectional,LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tree_stem import stem_word, word_to_vec\n",
    "from tokenize_uk import tokenize_words\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages_dataset.csv') # load dataset\n",
    "class_name = ['economics', 'politics', 'sports', 'entertainment', 'technology']\n",
    "ukrainian_stopwords = list(pd.read_csv(\"stopwords_ua.txt\", header=None, names=['stopwords']).stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_equally(df):\n",
    "    grouped = df.groupby(['topic'])\n",
    "    smallest = grouped.count().min().values\n",
    "    try: # Pandas 1.1.0+\n",
    "        return grouped.sample(smallest)\n",
    "    except AttributeError: # Pre-Pandas 1.1.0\n",
    "        return grouped.apply(lambda df: df.sample(smallest))\n",
    "df = get_dataset_equally(df)\n",
    "df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean dataset messages\n",
    "def clean_message(message):\n",
    "    text = re.sub(\"https?:\\/\\/[\\w+.\\/]+\", \" \", str(message))\n",
    "    text = re.sub('[^a-z–∞–±–≤–≥“ë–¥–µ—î–∂–∑–∏—ñ—ó–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—å—é—è]', ' ', str(text)).lower()\n",
    "    sen = re.sub(' +',' ',str(text))\n",
    "    words = tokenize_words(sen)\n",
    "    sen = []\n",
    "    for word in words:\n",
    "        if word not in ukrainian_stopwords:\n",
    "            try:\n",
    "                sen.append(stem_word(word))\n",
    "            except:\n",
    "                sen.append(word)\n",
    "\n",
    "    text = ' '.join(sen)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [] \n",
    "x = df.message\n",
    "y = df.topic\n",
    "\n",
    "for msg in x:\n",
    "    corpus.append(clean_message(msg))\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = tf.keras.models.load_model(\"lstm-bidir-250words-16batch-07-0.95.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–æ–≤ –∫–∞—Ñ afe mariia n n –∞–≤–ª—ñ–≤—Å—å–∫ —Ö –∏—Ä n —É—Ç —à—É–º–Ω–æ –ª—é–¥–Ω–æ –∂–∏–≤ –∞–∫ –º–∞ –º—ñ—Å—å–∫ –∫–∞—Ñ n —ñ–¥—á—É—Ç—Ç –Ω–∞–π–ø—Ä–µ–∫—Ä–∞—Å–Ω—ñ—à –∏—î–≤ –∑ –º—ñ—Å –≤–æ—Ä—á –ª–µ–≥–∫ —Ç–∞–ª–∞–Ω–æ–≤–∏—Ç –ª–µ—Ç—é—á —â–∞—Å–ª–∏–≤ –ø–æ–≥–ª—è–¥ –º—Ä—ñ –≤–µ–ª–∏–∫ —Ä–æ–∑–º–æ–≤ –∞–ø—Ä–æ—á—É–¥ –∞—Ä—ñ –≤–¥–∞ –∑—ñ–±—Ä–∞ –∫ —é–Ω—ñ—Ç –º—Ä—ñ –∑–∞–∫–ª–∞–¥ n —Ä–∏–º—ñ—â–µ–Ω–Ω –º–µ–±–ª –ø–µ—Ä–µ–π —Å–ø–∞–¥ –∫–∞—Ñ  –ø—Ä–æ–∂ –º–µ–Ω—à –Ω–æ–≤ —è–≤ –≤–µ–ª–∏–∫ –ø–µ–∫–∞—Ä—Å—å–∫ –≤—ñ—Ç—Ä–∏–Ω –∑–º—ñ–Ω —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω –ø–æ—Å–∞–¥ n –µ–Ω –∞—Ä—ñ –∫–æ–º—Ñ–æ—Ä—Ç—ñ–∫ —É—Ç –ª–µ–≥ –∑–æ—Ä—ñ—î–Ω—Ç—É –ø–æ–∑–∏—Ü—ñ –∑–Ω–∞–π–æ–º –∫–ª–∞—Å–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω —Å—Ç—Ä–∞ –¥–æ–ø–æ–º–∞–≥–∞ —à–µ—Ñ –∑–≤–∏—á–Ω —Ä–µ—Ü–µ–ø—Ç –∑–∞–≥—Ä–∞ –Ω–æ–≤ —Ñ–∞—Ä–± –∞–ø—Ä–∏–∫–ª–∞–¥ —Å–Ω—ñ–¥–∞–Ω –ø–æ–¥–∞ —Ç–µ–ª—è—á —è–∑–∏–∫ –≤–≤–µ—á–µ—Ä –≥–æ—Ç—É –∫—Ä—É–¥ —Ñ–µ–Ω n\n",
      "[[6688, 1705, 1, 56509, 9278, 9278, 1, 448, 1, 9278, 7992, 1, 1, 804, 12721, 20, 803, 1705, 9278, 1, 37019, 1, 14, 49, 1, 1163, 2765, 10516, 3337, 1109, 974, 46, 1128, 1, 19619, 530, 550, 666, 36472, 974, 533, 9278, 1, 6462, 3618, 3547, 1705, 8208, 561, 19, 10232, 46, 1, 14259, 70, 3541, 573, 9278, 11633, 19619, 1, 7992, 1590, 29165, 363, 1278, 10137, 377, 3994, 731, 4244, 3287, 3917, 8894, 19, 7671, 1, 5128, 607, 23018, 11227, 3606, 464, 1, 33048, 9278]]\n",
      "['–ù–æ–≤–µ –∫–∞—Ñ–µ Cafe mariia ‚ú®\\nüìç–î–µ?\\n–ü–∞–≤–ª—ñ–≤—Å—å–∫–∞, 26/41 (10 —Ö–≤ –≤—ñ–¥ –¶–∏—Ä–∫—É)\\n–¢—É—Ç —à—É–º–Ω–æ, –ª—é–¥–Ω–æ —ñ –¥—É–∂–µ –∂–∏–≤–æ. –¢–∞–∫–∏–º —ñ –º–∞—î –±—É—Ç–∏ –º—ñ—Å—å–∫–µ –∫–∞—Ñ–µ.\\n–í—ñ–¥—á—É—Ç—Ç—è, –Ω—ñ–±–∏ –≤—Å—ñ –Ω–∞–π–ø—Ä–µ–∫—Ä–∞—Å–Ω—ñ—à—ñ –ª—é–¥–∏ –ö–∏—î–≤–∞ –∑—ñ–±—Ä–∞–ª–∏—Å—å –≤ –æ–¥–Ω–æ–º—É –º—ñ—Å—Ü—ñ. –¢–≤–æ—Ä—á—ñ, –ª–µ–≥–∫—ñ, —Ç–∞–ª–∞–Ω–æ–≤–∏—Ç—ñ, –∑ –ª–µ—Ç—é—á–∏–º —â–∞—Å–ª–∏–≤–∏–º –ø–æ–≥–ª—è–¥–æ–º —Ç–∞ –º—Ä—ñ—è–º–∏ –ø—Ä–æ –≤–µ–ª–∏–∫–µ –≤ —Ä–æ–∑–º–æ–≤–∞—Ö. –ù–∞–ø—Ä–æ—á—É–¥ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ –ú–∞—Ä—ñ—ó –≤–¥–∞–ª–æ—Å—å –∑—ñ–±—Ä–∞—Ç–∏ –≤–ª–∞—Å–Ω–µ –∫–æ–º‚Äô—é–Ω—ñ—Ç—ñ, –ø—Ä–æ —è–∫–µ, –º–∞–±—É—Ç—å, –º—Ä—ñ—è–≤ –±–∏ –∫–æ–∂–Ω–∏–π –∑–∞–∫–ª–∞–¥‚ú®\\n–ü—Ä–∏–º—ñ—â–µ–Ω–Ω—è —Ç–∞ –º–µ–±–ª—ñ –ø–µ—Ä–µ–π—à–ª–∏ —É —Å–ø–∞–¥–æ–∫ –≤—ñ–¥ –∫–∞—Ñ–µ –ó–æ—Ä—è, —è–∫–∞ —Ç—É—Ç –ø—Ä–æ–∂–∏–ª–∞ –º–µ–Ω—à —è–∫ —Ä—ñ–∫. –ó –Ω–æ–≤–æ–≥–æ: –∑‚Äô—è–≤–∏–ª–∞—Å—å –≤–µ–ª–∏–∫–∞ –ø–µ–∫–∞—Ä—Å—å–∫–∞ –≤—ñ—Ç—Ä–∏–Ω–∞ —Ç–∞ —Ç—Ä–æ—Ö–∏ –∑–º—ñ–Ω–∏–ª–æ—Å—å —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è –ø–æ—Å–∞–¥–∫–∏.\\n–ú–µ–Ω—é –ú–∞—Ä—ñ—ó ‚Äî –∫–æ–º—Ñ–æ—Ä—Ç—ñ–∫. –¢—É—Ç –ª–µ–≥–∫–æ –∑–æ—Ä—ñ—î–Ω—Ç—É–≤–∞—Ç–∏—Å—å, –±–æ –≤—Å—ñ –ø–æ–∑–∏—Ü—ñ—ó –¥–∞–≤–Ω–æ –∑–Ω–∞–π–æ–º—ñ. –ê–ª–µ –∫–ª–∞—Å–Ω–æ, —â–æ —É —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ —Å—Ç—Ä–∞–≤ –¥–æ–ø–æ–º–∞–≥–∞–ª–∏ —à–µ—Ñ–∏ —ñ —Ç–æ–º—É –Ω–∞–≤—ñ—Ç—å –∑–≤–∏—á–Ω—ñ —Ä–µ—Ü–µ–ø—Ç–∏ –∑–∞–≥—Ä–∞–ª–∏ –Ω–æ–≤–∏–º–∏ —Ñ–∞—Ä–±–∞–º–∏. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –Ω–∞ —Å–Ω—ñ–¥–∞–Ω–æ–∫ –ø–æ–¥–∞—é—Ç—å —Ç–µ–ª—è—á–∏–π —è–∑–∏–∫, –∞ –≤–≤–µ—á–µ—Ä—ñ –≥–æ—Ç—É—é—Ç—å –∫—Ä—É–¥–æ –∑ —Ñ–µ–Ω—Ö–µ–ª–µ–º.\\n']\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "topic is: entertainment\n"
     ]
    }
   ],
   "source": [
    "text_input = [\"\"\"–ù–æ–≤–µ –∫–∞—Ñ–µ Cafe mariia ‚ú®\n",
    "üìç–î–µ?\n",
    "–ü–∞–≤–ª—ñ–≤—Å—å–∫–∞, 26/41 (10 —Ö–≤ –≤—ñ–¥ –¶–∏—Ä–∫—É)\n",
    "–¢—É—Ç —à—É–º–Ω–æ, –ª—é–¥–Ω–æ —ñ –¥—É–∂–µ –∂–∏–≤–æ. –¢–∞–∫–∏–º —ñ –º–∞—î –±—É—Ç–∏ –º—ñ—Å—å–∫–µ –∫–∞—Ñ–µ.\n",
    "–í—ñ–¥—á—É—Ç—Ç—è, –Ω—ñ–±–∏ –≤—Å—ñ –Ω–∞–π–ø—Ä–µ–∫—Ä–∞—Å–Ω—ñ—à—ñ –ª—é–¥–∏ –ö–∏—î–≤–∞ –∑—ñ–±—Ä–∞–ª–∏—Å—å –≤ –æ–¥–Ω–æ–º—É –º—ñ—Å—Ü—ñ. –¢–≤–æ—Ä—á—ñ, –ª–µ–≥–∫—ñ, —Ç–∞–ª–∞–Ω–æ–≤–∏—Ç—ñ, –∑ –ª–µ—Ç—é—á–∏–º —â–∞—Å–ª–∏–≤–∏–º –ø–æ–≥–ª—è–¥–æ–º —Ç–∞ –º—Ä—ñ—è–º–∏ –ø—Ä–æ –≤–µ–ª–∏–∫–µ –≤ —Ä–æ–∑–º–æ–≤–∞—Ö. –ù–∞–ø—Ä–æ—á—É–¥ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ –ú–∞—Ä—ñ—ó –≤–¥–∞–ª–æ—Å—å –∑—ñ–±—Ä–∞—Ç–∏ –≤–ª–∞—Å–Ω–µ –∫–æ–º‚Äô—é–Ω—ñ—Ç—ñ, –ø—Ä–æ —è–∫–µ, –º–∞–±—É—Ç—å, –º—Ä—ñ—è–≤ –±–∏ –∫–æ–∂–Ω–∏–π –∑–∞–∫–ª–∞–¥‚ú®\n",
    "–ü—Ä–∏–º—ñ—â–µ–Ω–Ω—è —Ç–∞ –º–µ–±–ª—ñ –ø–µ—Ä–µ–π—à–ª–∏ —É —Å–ø–∞–¥–æ–∫ –≤—ñ–¥ –∫–∞—Ñ–µ –ó–æ—Ä—è, —è–∫–∞ —Ç—É—Ç –ø—Ä–æ–∂–∏–ª–∞ –º–µ–Ω—à —è–∫ —Ä—ñ–∫. –ó –Ω–æ–≤–æ–≥–æ: –∑‚Äô—è–≤–∏–ª–∞—Å—å –≤–µ–ª–∏–∫–∞ –ø–µ–∫–∞—Ä—Å—å–∫–∞ –≤—ñ—Ç—Ä–∏–Ω–∞ —Ç–∞ —Ç—Ä–æ—Ö–∏ –∑–º—ñ–Ω–∏–ª–æ—Å—å —Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è –ø–æ—Å–∞–¥–∫–∏.\n",
    "–ú–µ–Ω—é –ú–∞—Ä—ñ—ó ‚Äî –∫–æ–º—Ñ–æ—Ä—Ç—ñ–∫. –¢—É—Ç –ª–µ–≥–∫–æ –∑–æ—Ä—ñ—î–Ω—Ç—É–≤–∞—Ç–∏—Å—å, –±–æ –≤—Å—ñ –ø–æ–∑–∏—Ü—ñ—ó –¥–∞–≤–Ω–æ –∑–Ω–∞–π–æ–º—ñ. –ê–ª–µ –∫–ª–∞—Å–Ω–æ, —â–æ —É —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ —Å—Ç—Ä–∞–≤ –¥–æ–ø–æ–º–∞–≥–∞–ª–∏ —à–µ—Ñ–∏ —ñ —Ç–æ–º—É –Ω–∞–≤—ñ—Ç—å –∑–≤–∏—á–Ω—ñ —Ä–µ—Ü–µ–ø—Ç–∏ –∑–∞–≥—Ä–∞–ª–∏ –Ω–æ–≤–∏–º–∏ —Ñ–∞—Ä–±–∞–º–∏. –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –Ω–∞ —Å–Ω—ñ–¥–∞–Ω–æ–∫ –ø–æ–¥–∞—é—Ç—å —Ç–µ–ª—è—á–∏–π —è–∑–∏–∫, –∞ –≤–≤–µ—á–µ—Ä—ñ –≥–æ—Ç—É—é—Ç—å –∫—Ä—É–¥–æ –∑ —Ñ–µ–Ω—Ö–µ–ª–µ–º.\n",
    "\"\"\"]\n",
    "opt_len = 250\n",
    "\n",
    "# clean input message\n",
    "text = clean_message(text_input)\n",
    "\n",
    "print(text)\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([text])\n",
    "print(sequence)\n",
    "embedding_text = pad_sequences(sequence,padding='post',maxlen=opt_len)\n",
    "print(text_input)\n",
    "print('topic is: {}'.format(class_name[np.argmax(model.predict([embedding_text]))]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
